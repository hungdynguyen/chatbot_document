# import os
# import glob
# from typing import List
# from uuid import uuid4

# from huggingface_hub import hf_hub_download
# from langchain_community.document_loaders import UnstructuredFileLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import Qdrant
# from qdrant_client import QdrantClient
# from qdrant_client.http.models import Distance, VectorParams
# from tqdm import tqdm

# from config import (
#     UPLOAD_DIRECTORY,
#     QDRANT_HOST,
#     QDRANT_PORT,
#     EMBEDDING_MODEL_NAME,
#     EMBEDDING_DEVICE,
#     CHUNK_SIZE,
#     CHUNK_OVERLAP
# )

# # ‚Äî‚Äî Chu·∫©n b·ªã cache_dir v√† ch·ªâ download nh·ªØng file c·∫ßn ‚Äî‚Äî #
# cache_dir = os.path.join(UPLOAD_DIRECTORY, ".cache", EMBEDDING_MODEL_NAME.replace("/", "_"))
# os.makedirs(cache_dir, exist_ok=True)

# print("üß† ƒêang cache model safetensors + config/tokenizer‚Ä¶")
# # Tr·ªçng s·ªë
# hf_hub_download(
#     repo_id=EMBEDDING_MODEL_NAME,
#     filename="model.safetensors",
#     cache_dir=cache_dir,
# )
# # Config
# hf_hub_download(
#     repo_id=EMBEDDING_MODEL_NAME,
#     filename="config.json",
#     cache_dir=cache_dir,
# )
# # Tokenizer (tu·ª≥ model c√≥ th·ªÉ l√† tokenizer.json, vocab.txt, merges.txt‚Ä¶)
# hf_hub_download(
#     repo_id=EMBEDDING_MODEL_NAME,
#     filename="tokenizer.json",
#     cache_dir=cache_dir,
# )

# # ‚Äî‚Äî Kh·ªüi t·∫°o embedding model t·ª´ cache ‚Äî‚Äî #
# print("üß† ƒêang kh·ªüi t·∫°o embedding model t·ª´ cache‚Ä¶")
# try:
#     embedding_model = HuggingFaceEmbeddings(
#         model_name=cache_dir,
#         model_kwargs={"device": EMBEDDING_DEVICE}
#     )
#     print("‚úÖ Embedding model ƒë√£ s·∫µn s√†ng.")
# except Exception as e:
#     print(f"‚ùå L·ªói kh·ªüi t·∫°o embedding model: {e}")
#     embedding_model = None

# # Kh·ªüi Qdrant client nh∆∞ c≈©
# qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)


# async def embed_files_to_qdrant(file_ids: List[str]) -> str:
#     """
#     ƒê·ªçc c√°c file t·ª´ UPLOAD_DIRECTORY, chia nh·ªè, embedding v√† ƒë·∫©y v√†o m·ªôt collection Qdrant M·ªöI.
#     Tr·∫£ v·ªÅ t√™n c·ªßa collection m·ªõi ƒë∆∞·ª£c t·∫°o.
#     """
#     if not embedding_model:
#         raise ValueError("Embedding model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng.")

#     # --- 1. T·∫°o m·ªôt collection duy nh·∫•t cho phi√™n l√†m vi·ªác n√†y ---
#     collection_name = f"session-{uuid4()}"
#     print(f"üöÄ T·∫°o collection Qdrant m·ªõi cho phi√™n l√†m vi·ªác: {collection_name}")
    
#     # L·∫•y dimension c·ªßa vector t·ª´ model
#     try:
#         # Th·ª≠ v·ªõi attribute client tr∆∞·ªõc
#         if hasattr(embedding_model, 'client'):
#             embed_dim = embedding_model.client.get_sentence_embedding_dimension()
#         else:
#             # Fallback: encode m·ªôt c√¢u sample ƒë·ªÉ l·∫•y dimension
#             sample_embedding = embedding_model.embed_query("test")
#             embed_dim = len(sample_embedding)
#     except Exception as e:
#         print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l·∫•y dimension t·ª± ƒë·ªông, s·ª≠ d·ª•ng 384 (default): {e}")
#         embed_dim = 384

#     qdrant_client.recreate_collection(
#         collection_name=collection_name,
#         vectors_config=VectorParams(size=embed_dim, distance=Distance.COSINE)
#     )

#     # --- 2. T√¨m v√† t·∫£i c√°c file ƒë∆∞·ª£c y√™u c·∫ßu ---
#     all_documents = []
#     print(f"üîç ƒêang t√¨m v√† t·∫£i {len(file_ids)} file...")
    
#     # T√¨m t·∫•t c·∫£ c√°c file trong th∆∞ m·ª•c upload
#     # C√°ch n√†y ƒë∆°n gi·∫£n nh∆∞ng kh√¥ng hi·ªáu qu·∫£ n·∫øu c√≥ h√†ng ngh√¨n file
#     # M·ªôt c√°ch t·ªët h∆°n l√† l∆∞u √°nh x·∫° file_id -> file_path v√†o DB
#     all_uploaded_files = glob.glob(os.path.join(UPLOAD_DIRECTORY, "*"))
    
#     target_files = []
#     for file_id in file_ids:
#         # T√¨m file c√≥ t√™n b·∫Øt ƒë·∫ßu b·∫±ng file_id
#         found = next((f for f in all_uploaded_files if os.path.basename(f).startswith(file_id)), None)
#         if found:
#             target_files.append(found)
#         else:
#             print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file cho ID: {file_id}")
    
#     if not target_files:
#         print("‚ùå Kh√¥ng c√≥ file n√†o h·ª£p l·ªá ƒë·ªÉ embedding.")
#         return collection_name # Tr·∫£ v·ªÅ t√™n collection r·ªóng

#     for file_path in tqdm(target_files, desc="ƒêang t·∫£i n·ªôi dung file"):
#         try:
#             # UnstructuredFileLoader c√≥ th·ªÉ ƒë·ªçc nhi·ªÅu lo·∫°i file (pdf, docx, txt...)
#             loader = UnstructuredFileLoader(file_path)
#             all_documents.extend(loader.load())
#         except Exception as e:
#             print(f"L·ªói khi t·∫£i file {file_path}: {e}")

#     # --- 3. Chia nh·ªè vƒÉn b·∫£n ---
#     print(f"üîÑ ƒêang chia nh·ªè {len(all_documents)} document...")
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=CHUNK_SIZE,
#         chunk_overlap=CHUNK_OVERLAP,
#     )
#     docs_for_db = text_splitter.split_documents(all_documents)
#     print(f"‚úÖ ƒê√£ chia th√†nh {len(docs_for_db)} chunk.")

#     # --- 4. Th√™m c√°c chunk v√†o Qdrant ---
#     if not docs_for_db:
#         print("‚ö†Ô∏è Kh√¥ng c√≥ chunk n√†o ƒë∆∞·ª£c t·∫°o ra ƒë·ªÉ embedding.")
#         return collection_name

#     print("‚è≥ ƒêang th·ª±c hi·ªán embedding v√† l∆∞u v√†o Qdrant...")
#     qdrant_vector_store = Qdrant(
#         client=qdrant_client,
#         collection_name=collection_name,
#         embeddings=embedding_model
#     )
#     qdrant_vector_store.add_documents(documents=docs_for_db, batch_size=64)

#     print(f"‚úÖ ƒê√£ embedding th√†nh c√¥ng v√†o collection '{collection_name}'.")
#     return collection_name

# backend/utils/embedding_handler.py
import os, glob
from typing import List
from uuid import uuid4

from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams
from tqdm import tqdm

from config import (
    UPLOAD_DIRECTORY,
    QDRANT_HOST, QDRANT_PORT,
    EMBEDDING_MODEL_NAME,     # = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    EMBEDDING_DEVICE,         # = "cpu" | "cuda"
    CHUNK_SIZE, CHUNK_OVERLAP
)

# 0Ô∏è‚É£  Kh·ªüi t·∫°o model 1 l·∫ßn
print("üß† ƒêang t·∫£i / kh·ªüi t·∫°o embedding model ‚Ä¶")
try:
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={"device": EMBEDDING_DEVICE},
    )
    print("‚úÖ Embedding model s·∫µn s√†ng.")
except Exception as e:
    print(f"‚ùå Kh√¥ng kh·ªüi t·∫°o ƒë∆∞·ª£c model: {e}")
    embedding_model = None

# 1Ô∏è‚É£  Qdrant client
qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)


async def embed_files_to_qdrant(file_ids: List[str]) -> str:
    """
    ƒê·ªçc file, chunk, embed, ƒë·∫©y v√†o collection m·ªõi. Tr·∫£ v·ªÅ t√™n collection.
    """
    if embedding_model is None:
        raise RuntimeError("Embedding model ch∆∞a s·∫µn s√†ng")

    # A. t·∫°o collection m·ªõi
    collection_name = f"session-{uuid4()}"
    print(f"üöÄ New collection: {collection_name}")

    # dimension
    try:
        dim = embedding_model.client.get_sentence_embedding_dimension()
    except Exception:
        dim = len(embedding_model.embed_query("test"))
    qdrant_client.recreate_collection(
        collection_name,
        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
    )

    # B. gom file theo file_ids
    uploaded_files = glob.glob(os.path.join(UPLOAD_DIRECTORY, "*"))
    target_files = [
        f for fid in file_ids
        for f in uploaded_files
        if os.path.basename(f).startswith(fid)
    ]
    if not target_files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file h·ª£p l·ªá.")
        return collection_name

    # C. load & chunk
    docs_raw = []
    for fp in tqdm(target_files, desc="ƒê·ªçc file"):
        try:
            docs_raw.extend(UnstructuredFileLoader(fp).load())
        except Exception as e:
            print(f"L·ªói {fp}: {e}")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
    )
    docs = splitter.split_documents(docs_raw)
    if not docs:
        print("‚ö†Ô∏è Kh√¥ng c√≥ chunk.")
        return collection_name

    # D. ghi v√†o Qdrant
    print("‚è≥ Embedding & ghi Qdrant ‚Ä¶")
    qstore = Qdrant(
        client=qdrant_client,
        collection_name=collection_name,
        embeddings=embedding_model,
    )
    qstore.add_documents(docs, batch_size=64)
    print(f"‚úÖ Ho√†n t·∫•t collection {collection_name}")
    return collection_name
