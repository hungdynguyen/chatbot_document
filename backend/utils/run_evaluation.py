import os
import json
import requests
import pandas as pd
import time
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
from deepdiff import DeepDiff
from datetime import datetime
# --- 1. C·∫§U H√åNH ---
BACKEND_URL = "http://localhost:8000"
DOCS_DIRECTORY = "/home/locmt/Techcombank_/chatbot_document/data/data_real" 
GROUND_TRUTH_PATH = "/home/locmt/Techcombank_/chatbot_document/backend/schemas/ground_truth_template4.json" 

# Th∆∞ m·ª•c l∆∞u tr·ªØ k·∫øt qu·∫£ evaluation
BASE_EVALUATION_DIR = "/home/locmt/Techcombank_/chatbot_document/evaluation_results"
REPORTS_DIR = os.path.join(BASE_EVALUATION_DIR, "reports")
RAW_DATA_DIR = os.path.join(BASE_EVALUATION_DIR, "raw_data")
os.makedirs(REPORTS_DIR, exist_ok=True)
os.makedirs(RAW_DATA_DIR, exist_ok=True)

SOURCE_FILES_FOR_TEST = [
    "call-rp.xlsx",
    "DN HMTD CAG 2024 (1).docx",
    "MB01-HD.SPDN_43 - H·ªó Tr·ª£ Ph√¢n Nh√≥m Kh√°ch H√†ng (1).xlsx",
    "SO-S√ÅNH-DN-C√ôNG-NG√ÄNH.docx",
    # "DKKD lan 8 ngay 12.04.2023.pdf" # Th√™m file n√†y n·∫øu c√≥
]

# C·∫•u h√¨nh Ragas (kh·ªõp v·ªõi embedding_handler.py)
from qdrant_client import QdrantClient
from langchain_huggingface import HuggingFaceEmbeddings
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)


# --- 2. C√ÅC H√ÄM T√çNH TO√ÅN METRICS ---

def calculate_extraction_accuracy(ground_truth_json, extracted_json):
    """
    S·ª≠ d·ª•ng DeepDiff ƒë·ªÉ so s√°nh 2 JSON v√† t√≠nh to√°n ƒë·ªô ch√≠nh x√°c.
    Metric quan tr·ªçng nh·∫•t c·ªßa b·∫°n.
    """
    diff = DeepDiff(ground_truth_json, extracted_json, ignore_order=True, verbose_level=0)
    
    total_fields = 0
    correct_fields = 0

    def count_fields(d):
        count = 0
        for k, v in d.items():
            if isinstance(v, dict):
                count += count_fields(v)
            elif isinstance(v, list):
                # Coi m·ªói item trong list l√† 1 field
                count += len(v) if v else 1 
            else:
                count += 1
        return count

    total_fields = count_fields(ground_truth_json)
    
    # S·ªë tr∆∞·ªùng b·ªã sai l√† t·ªïng s·ªë thay ƒë·ªïi, th√™m, b·ªõt
    errors = len(diff.get('values_changed', {})) + \
             len(diff.get('dictionary_item_added', {})) + \
             len(diff.get('dictionary_item_removed', {}))
    
    correct_fields = total_fields - errors
    accuracy = (correct_fields / total_fields) * 100 if total_fields > 0 else 0
    
    return accuracy, diff

def generate_ragas_dataset(ground_truth_json, extracted_json, collection_name):
    """
    T·∫°o dataset cho Ragas t·ª´ ground truth v√† k·∫øt qu·∫£ tr√≠ch xu·∫•t.
    """
    ragas_data = {"question": [], "answer": [], "contexts": [], "ground_truths": []}
    
    # Helper function ƒë·ªÉ duy·ªát qua JSON l·ªìng nhau
    def flatten_and_ask(node, path=""):
        for key, value in node.items():
            current_path = f"{path}.{key}" if path else key
            if isinstance(value, dict):
                flatten_and_ask(value, current_path)
            elif isinstance(value, list) and value and isinstance(value[0], dict):
                 # X·ª≠ l√Ω cho b·∫£ng (v√≠ d·ª• banLanhDao)
                 question = f"H√£y cung c·∫•p th√¥ng tin chi ti·∫øt v·ªÅ: {current_path}"
                 ground_truth_answer = json.dumps(value, ensure_ascii=False)
                 extracted_answer_obj = extracted_json
                 for p in current_path.split('.'): extracted_answer_obj = extracted_answer_obj.get(p, {})
                 extracted_answer = json.dumps(extracted_answer_obj, ensure_ascii=False)
                 contexts = retrieve_contexts(question, collection_name)

                 ragas_data["question"].append(question)
                 ragas_data["answer"].append(extracted_answer)
                 ragas_data["contexts"].append(contexts)
                 ragas_data["ground_truths"].append([ground_truth_answer])
            elif value is not None:
                question = f"Th√¥ng tin v·ªÅ '{current_path}' l√† g√¨?"
                ground_truth_answer = str(value)
                
                extracted_answer_obj = extracted_json
                try:
                    for p in current_path.split('.'): extracted_answer_obj = extracted_answer_obj.get(p, "")
                except:
                    extracted_answer_obj = ""

                extracted_answer = str(extracted_answer_obj)
                contexts = retrieve_contexts(question, collection_name)

                ragas_data["question"].append(question)
                ragas_data["answer"].append(extracted_answer)
                ragas_data["contexts"].append(contexts)
                ragas_data["ground_truths"].append([ground_truth_answer])

    flatten_and_ask(ground_truth_json)
    return Dataset.from_dict(ragas_data)

def retrieve_contexts(question, collection_name):
    """Truy xu·∫•t ng·ªØ c·∫£nh t·ª´ Qdrant (c·∫ßn cho Ragas)."""
    try:
        query_vector = embedding_model.embed_query(question)
        hits = qdrant_client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=10 # Ph·∫£i kh·ªõp v·ªõi Number of Results trong Langflow
        )
        return [hit.payload["page_content"] for hit in hits]
    except Exception as e:
        print(f"L·ªói khi truy xu·∫•t ng·ªØ c·∫£nh t·ª´ Qdrant: {e}")
        return []
    

def save_evaluation_results(accuracy, latency, ragas_scores, diff_details, extracted_json, ground_truth_json):
    """L∆∞u k·∫øt qu·∫£ evaluation v√†o file Excel v·ªõi timestamp."""
    
    # T·∫°o timestamp cho t√™n file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"evaluation_report_{timestamp}.xlsx"
    filepath = os.path.join(REPORTS_DIR, filename)
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu cho c√°c sheet
    with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
        
        # Sheet 1: Summary Metrics
        hallucination_rate = 1.0 - ragas_scores.get('faithfulness', 0)
        summary_data = {
            "Metric": [
                "Extraction Accuracy (%)",
                "End-to-End Latency (s)",
                "Faithfulness",
                "Context Precision", 
                "Context Recall",
                "Answer Relevancy",
                "Hallucination Rate (%)"
            ],
            "Score": [
                f"{accuracy:.2f}",
                f"{latency:.2f}",
                f"{ragas_scores.get('faithfulness', 0):.4f}",
                f"{ragas_scores.get('context_precision', 0):.4f}",
                f"{ragas_scores.get('context_recall', 0):.4f}",
                f"{ragas_scores.get('answer_relevancy', 0):.4f}",
                f"{hallucination_rate:.2%}"
            ],
            "Timestamp": [timestamp] * 7,
            "Template": ["template4"] * 7
        }
        
        df_summary = pd.DataFrame(summary_data)
        df_summary.to_excel(writer, sheet_name='Summary', index=False)
        
        # Sheet 2: Detailed Comparison
        comparison_data = []
        
        def flatten_json_for_comparison(gt_node, ext_node, path=""):
            """Flatten JSON ƒë·ªÉ so s√°nh chi ti·∫øt t·ª´ng field."""
            for key, gt_value in gt_node.items():
                current_path = f"{path}.{key}" if path else key
                ext_value = ext_node.get(key, "") if isinstance(ext_node, dict) else ""
                
                if isinstance(gt_value, dict) and isinstance(ext_value, dict):
                    flatten_json_for_comparison(gt_value, ext_value, current_path)
                elif isinstance(gt_value, list):
                    comparison_data.append({
                        "Field": current_path,
                        "Ground Truth": json.dumps(gt_value, ensure_ascii=False),
                        "Extracted": json.dumps(ext_value, ensure_ascii=False),
                        "Match": "‚úì" if gt_value == ext_value else "‚úó",
                        "Type": "List"
                    })
                else:
                    comparison_data.append({
                        "Field": current_path,
                        "Ground Truth": str(gt_value),
                        "Extracted": str(ext_value),
                        "Match": "‚úì" if str(gt_value) == str(ext_value) else "‚úó",
                        "Type": type(gt_value).__name__
                    })
        
        flatten_json_for_comparison(ground_truth_json, extracted_json)
        
        if comparison_data:
            df_comparison = pd.DataFrame(comparison_data)
            df_comparison.to_excel(writer, sheet_name='Field_Comparison', index=False)
        
        # Sheet 3: Diff Details (n·∫øu c√≥)
        if diff_details:
            diff_data = []
            
            # Values changed
            if 'values_changed' in diff_details:
                for path, change in diff_details['values_changed'].items():
                    diff_data.append({
                        "Type": "Value Changed",
                        "Path": path,
                        "Old Value": str(change.get('old_value', '')),
                        "New Value": str(change.get('new_value', ''))
                    })
            
            # Items added
            if 'dictionary_item_added' in diff_details:
                for path in diff_details['dictionary_item_added']:
                    diff_data.append({
                        "Type": "Item Added",
                        "Path": str(path),
                        "Old Value": "N/A",
                        "New Value": "Added"
                    })
            
            # Items removed
            if 'dictionary_item_removed' in diff_details:
                for path in diff_details['dictionary_item_removed']:
                    diff_data.append({
                        "Type": "Item Removed", 
                        "Path": str(path),
                        "Old Value": "Removed",
                        "New Value": "N/A"
                    })
            
            if diff_data:
                df_diff = pd.DataFrame(diff_data)
                df_diff.to_excel(writer, sheet_name='Diff_Details', index=False)
        
        # Sheet 4: Raw Data
        raw_data = {
            "Ground Truth JSON": [json.dumps(ground_truth_json, ensure_ascii=False, indent=2)],
            "Extracted JSON": [json.dumps(extracted_json, ensure_ascii=False, indent=2)],
            "Evaluation Time": [timestamp],
            "Files Used": [", ".join(SOURCE_FILES_FOR_TEST)]
        }
        
        df_raw = pd.DataFrame(raw_data)
        df_raw.to_excel(writer, sheet_name='Raw_Data', index=False)
    
    print(f"üìä K·∫øt qu·∫£ evaluation ƒë√£ ƒë∆∞·ª£c l∆∞u: {filepath}")
    return filepath

# --- 3. H√ÄM CH√çNH ƒêI·ªÄU PH·ªêI ---

def run_full_evaluation():
    """H√†m ch√≠nh ƒëi·ªÅu ph·ªëi to√†n b·ªô quy tr√¨nh ƒë√°nh gi√°."""
    
    # --- GIAI ƒêO·∫†N 1: CHU·∫®N B·ªä ---
    print("--- Giai ƒëo·∫°n 1: Chu·∫©n b·ªã m√¥i tr∆∞·ªùng ---")
    
    # 1.1 T·∫£i ground truth
    with open(GROUND_TRUTH_PATH, 'r', encoding='utf-8') as f:
        ground_truth_json = json.load(f)
        
    # 1.2 Upload c√°c file c·∫ßn thi·∫øt
    print("Uploading source files...")
    file_id_map = {}
    for filename in SOURCE_FILES_FOR_TEST:
        filepath = os.path.join(DOCS_DIRECTORY, filename)
        if os.path.exists(filepath):
            file_id = requests.post(f"{BACKEND_URL}/upload_file", files={"file": open(filepath, "rb")}).json()["file_id"]
            file_id_map[filename] = file_id
        else:
            print(f"‚ö†Ô∏è File not found: {filepath}")

    # --- GIAI ƒêO·∫†N 2: CH·∫†Y H·ªÜ TH·ªêNG V√Ä ƒêO L∆Ø·ªúNG ---
    print("\n--- Giai ƒëo·∫°n 2: Ch·∫°y tr√≠ch xu·∫•t v√† ƒëo l∆∞·ªùng Latency ---")
    
    payload = {
        "prompt": "Tr√≠ch xu·∫•t th√¥ng tin theo m·∫´u B√°o c√°o th·∫©m ƒë·ªãnh.",
        "file_ids": list(file_id_map.values()),
        "template_id": "template4"
    }
    
    start_time = time.time()
    response = requests.post(f"{BACKEND_URL}/process_prompt", json=payload, timeout=300)
    end_time = time.time()
    
    if response.status_code != 200:
        print(f"‚ùå L·ªói API nghi√™m tr·ªçng: {response.text}")
        return

    extracted_result = response.json()
    extracted_json = extracted_result["extracted_data"]
    collection_name = extracted_result["collection_name"]
    
    # Metric: End-to-End Latency
    latency = end_time - start_time
    print(f"‚úÖ Tr√≠ch xu·∫•t ho√†n t·∫•t. Collection ƒë∆∞·ª£c s·ª≠ d·ª•ng: {collection_name}")
    print(f"‚è±Ô∏è End-to-End Latency: {latency:.2f} gi√¢y")

    # --- GIAI ƒêO·∫†N 3: T√çNH TO√ÅN METRICS ---
    print("\n--- Giai ƒëo·∫°n 3: T√≠nh to√°n c√°c metrics ---")

    # Metric: Extraction Accuracy
    accuracy, diff = calculate_extraction_accuracy(ground_truth_json, extracted_json)
    print(f"üéØ Extraction Accuracy: {accuracy:.2f}%")
        
    # Chu·∫©n b·ªã dataset cho Ragas
    ragas_dataset = generate_ragas_dataset(ground_truth_json, extracted_json, collection_name)
    
    # ƒê√°nh gi√° b·∫±ng Ragas
    print("\n   -> ƒêang ch·∫°y ƒë√°nh gi√° Ragas (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...")
    ragas_result = evaluate(
        ragas_dataset,
        metrics=[context_precision, context_recall, faithfulness, answer_relevancy],
    )
    
    # --- GIAI ƒêO·∫†N 4: HI·ªÇN TH·ªä K·∫æT QU·∫¢ T·ªîNG H·ª¢P ---
    print("\n--- B·∫¢NG ƒêI·ªÄU KHI·ªÇN ƒê√ÅNH GI√Å ---")
    
    ragas_scores = ragas_result.scores
    hallucination_rate = 1.0 - ragas_scores.get('faithfulness', 0)
    
    summary = {
        "Metric": [
            "Extraction Accuracy (%)",
            "End-to-End Latency (s)",
            "--- Ragas Core Metrics ---",
            "Faithfulness",
            "Context Precision",
            "Context Recall",
            "Answer Relevancy",
            "--- Reliability Metrics ---",
            "Hallucination Rate (%)"
        ],
        "Score": [
            f"{accuracy:.2f}",
            f"{latency:.2f}",
            "--------------------------",
            f"{ragas_scores.get('faithfulness', 'N/A'):.4f}",
            f"{ragas_scores.get('context_precision', 'N/A'):.4f}",
            f"{ragas_scores.get('context_recall', 'N/A'):.4f}",
            f"{ragas_scores.get('answer_relevancy', 'N/A'):.4f}",
            "--------------------------",
            f"{hallucination_rate:.2%}"
        ]
    }
    
    df_summary = pd.DataFrame(summary)
    print(df_summary.to_string(index=False))
    
    # --- GIAI ƒêO·∫†N 5: LUU K·∫æT QU·∫¢ V√ÄO EXCEL ---
    print("\n--- Giai ƒëo·∫°n 5: L∆∞u k·∫øt qu·∫£ v√†o Excel ---")
    excel_filepath = save_evaluation_results(
        accuracy=accuracy,
        latency=latency, 
        ragas_scores=ragas_scores,
        diff_details=diff,
        extracted_json=extracted_json,
        ground_truth_json=ground_truth_json
    )

    # --- GIAI ƒêO·∫†N 6: D·ªåN D·∫∏P ---
    requests.post(f"{BACKEND_URL}/clear_rag_session", json={"collection_name": collection_name})
    print(f"\n‚úÖ ƒê√£ d·ªçn d·∫πp collection: {collection_name}")


if __name__ == "__main__":
    run_full_evaluation()