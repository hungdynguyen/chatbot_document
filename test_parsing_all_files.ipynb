{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a64ba6d",
   "metadata": {},
   "source": [
    "# Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53505436",
   "metadata": {},
   "source": [
    "### List files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31702331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Excel files found: 8\n",
      "Excel files found in /mnt/d/Techcombank_/chatbot_document/data/data_real:\n",
      "Bao_cao_tai_chinh_tong_hop-1-1.xlsx\n",
      "BCTD-4.xlsx\n",
      "call-rp.xlsx\n",
      "Cong-cu-danh-gia-RR-MTXH-_ES-Tool_Biz-CAG.xlsx\n",
      "dien-giai-131_154-cham-luan-chuyen.xlsx\n",
      "phan-tich-ton-kho-phai-thu-phai-tra.xlsx\n",
      "tong-hop.xlsx\n",
      "~$BCTD-4.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "folder_path = \"/mnt/d/Techcombank_/chatbot_document/data/data_real\"\n",
    "\n",
    "# List all Excel files in the folder\n",
    "excel_files = glob.glob(os.path.join(folder_path, \"*.xlsx\")) + glob.glob(os.path.join(folder_path, \"*.xls\"))\n",
    "print(f\"Total Excel files found: {len(excel_files)}\")\n",
    "print(f\"Excel files found in {folder_path}:\")\n",
    "for file in excel_files:\n",
    "    print(os.path.basename(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88803ab6",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fbe64fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: /mnt/d/Techcombank_/chatbot_document/data/data_real/Bao_cao_tai_chinh_tong_hop-1-1.xlsx\n",
      "Parsed 57 documents from /mnt/d/Techcombank_/chatbot_document/data/data_real/Bao_cao_tai_chinh_tong_hop-1-1.xlsx\n",
      "Parsing file: /mnt/d/Techcombank_/chatbot_document/data/data_real/BCTD-4.xlsx\n",
      "Error parsing Excel file (optimized) /mnt/d/Techcombank_/chatbot_document/data/data_real/BCTD-4.xlsx: [Errno 2] No such file or directory: '/mnt/d/Techcombank_/chatbot_document/data/data_real/BCTD-4.xlsx'\n",
      "Parsed 0 documents from /mnt/d/Techcombank_/chatbot_document/data/data_real/BCTD-4.xlsx\n",
      "Parsing file: /mnt/d/Techcombank_/chatbot_document/data/data_real/call-rp.xlsx\n",
      "Parsed 104 documents from /mnt/d/Techcombank_/chatbot_document/data/data_real/call-rp.xlsx\n",
      "Parsing file: /mnt/d/Techcombank_/chatbot_document/data/data_real/Cong-cu-danh-gia-RR-MTXH-_ES-Tool_Biz-CAG.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Techcombank_/chatbot_document/venv/lib/python3.12/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/mnt/d/Techcombank_/chatbot_document/venv/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/mnt/d/Techcombank_/chatbot_document/venv/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/mnt/d/Techcombank_/chatbot_document/venv/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/mnt/d/Techcombank_/chatbot_document/venv/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/mnt/d/Techcombank_/chatbot_document/venv/lib/python3.12/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 785 documents from /mnt/d/Techcombank_/chatbot_document/data/data_real/Cong-cu-danh-gia-RR-MTXH-_ES-Tool_Biz-CAG.xlsx\n",
      "Parsing file: /mnt/d/Techcombank_/chatbot_document/data/data_real/dien-giai-131_154-cham-luan-chuyen.xlsx\n",
      "Parsed 115 documents from /mnt/d/Techcombank_/chatbot_document/data/data_real/dien-giai-131_154-cham-luan-chuyen.xlsx\n",
      "Parsing file: /mnt/d/Techcombank_/chatbot_document/data/data_real/phan-tich-ton-kho-phai-thu-phai-tra.xlsx\n",
      "Parsed 235 documents from /mnt/d/Techcombank_/chatbot_document/data/data_real/phan-tich-ton-kho-phai-thu-phai-tra.xlsx\n",
      "Parsing file: /mnt/d/Techcombank_/chatbot_document/data/data_real/tong-hop.xlsx\n",
      "Parsed 13 documents from /mnt/d/Techcombank_/chatbot_document/data/data_real/tong-hop.xlsx\n",
      "Parsing file: /mnt/d/Techcombank_/chatbot_document/data/data_real/~$BCTD-4.xlsx\n",
      "Error parsing Excel file (optimized) /mnt/d/Techcombank_/chatbot_document/data/data_real/~$BCTD-4.xlsx: [Errno 2] No such file or directory: '/mnt/d/Techcombank_/chatbot_document/data/data_real/~$BCTD-4.xlsx'\n",
      "Parsed 0 documents from /mnt/d/Techcombank_/chatbot_document/data/data_real/~$BCTD-4.xlsx\n",
      "Total documents created: 1309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Techcombank_/chatbot_document/venv/lib/python3.12/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/mnt/d/Techcombank_/chatbot_document/venv/lib/python3.12/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def parse_excel(file_path: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Parse Excel file một cách tối ưu, biến mỗi hàng của mỗi bảng thành một Document riêng biệt\n",
    "    với metadata chi tiết.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    try:\n",
    "        # Sử dụng ExcelFile để có thể truy cập các sheet hiệu quả\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        \n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "            \n",
    "            # --- Xử lý cho sheet rỗng ---\n",
    "            if df.empty:\n",
    "                continue\n",
    "                \n",
    "            # --- Logic để xác định các bảng riêng biệt trong một sheet ---\n",
    "            df['is_empty'] = df.isnull().all(axis=1)\n",
    "            df['table_id'] = df['is_empty'].cumsum()\n",
    "            \n",
    "            tables = df.groupby('table_id')\n",
    "            \n",
    "            for table_id, table_df in tables:\n",
    "                # Bỏ các dòng trống đã dùng làm dấu phân cách\n",
    "                table_df = table_df.dropna(how='all').reset_index(drop=True)\n",
    "                table_df = table_df.drop(columns=['is_empty', 'table_id'], errors='ignore')\n",
    "\n",
    "                if table_df.empty:\n",
    "                    continue\n",
    "                \n",
    "                headers = table_df.columns.tolist()\n",
    "\n",
    "                for index, row in table_df.iterrows():\n",
    "                    row_texts = [\n",
    "                        f\"{str(col_name).strip()}: {str(row[col_name]).strip()}\"\n",
    "                        for col_name in headers if pd.notna(row[col_name])\n",
    "                    ]\n",
    "                    \n",
    "                    if not row_texts:\n",
    "                        continue\n",
    "                        \n",
    "                    page_content = \" | \".join(row_texts)\n",
    "                    \n",
    "                    metadata = {\n",
    "                        \"source\": str(file_path),\n",
    "                        \"file_type\": \"excel\",\n",
    "                        \"sheet_name\": sheet_name,\n",
    "                        \"table_id\": f\"table_{table_id}\",\n",
    "                        \"row_index_in_table\": index\n",
    "                    }\n",
    "                    \n",
    "                    documents.append(Document(\n",
    "                        page_content=page_content,\n",
    "                        metadata=metadata\n",
    "                    ))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing Excel file (optimized) {file_path}: {e}\")\n",
    "            \n",
    "    return documents\n",
    "\n",
    "# Parse all Excel files in the folder\n",
    "if __name__ == \"__main__\":\n",
    "    all_documents = []\n",
    "    for file_path in excel_files:\n",
    "        file_path = Path(file_path)\n",
    "        print(f\"Parsing file: {file_path}\")\n",
    "        documents = parse_excel(file_path)\n",
    "        all_documents.extend(documents)\n",
    "        #Save temporary results to avoid losing progress\n",
    "        print(f\"Parsed {len(documents)} documents from {file_path}\")\n",
    "    # Save all documents to JSON file with descriptive name\n",
    "    output_file = Path(\"/mnt/d/Techcombank_/chatbot_document/data/output/excel_documents_parsed.json\")\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in all_documents], f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Total documents created: {len(all_documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36059816",
   "metadata": {},
   "source": [
    "# PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ffcaa",
   "metadata": {},
   "source": [
    "### List files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c817dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDF files found: 0\n",
      "PDF files found in /mnt/d/Techcombank_/chatbot_document/data/data_real:\n"
     ]
    }
   ],
   "source": [
    "# List all PDF files in the folder\n",
    "pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
    "print(f\"Total PDF files found: {len(pdf_files)}\")\n",
    "print(f\"PDF files found in {folder_path}:\")\n",
    "for file in pdf_files:\n",
    "    print(os.path.basename(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e0b891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents created: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF - cần cho pdf2image\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def parse_pdf(file_path: Path) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Optimized PDF parser: extracts tables row-by-row and text paragraph-by-paragraph.\n",
    "        Includes OCR fallback.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a text splitter for chunking large texts\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "        )\n",
    "        documents = []\n",
    "        \n",
    "        # --- Method 1: Direct text and table extraction ---\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                    # 1. Extract tables first\n",
    "                    tables = page.extract_tables()\n",
    "                    for table_idx, table in enumerate(tables):\n",
    "                        # Giả định hàng đầu tiên là header\n",
    "                        headers = [h.strip() if h else f\"col_{i}\" for i, h in enumerate(table[0])]\n",
    "                        for row_idx, row in enumerate(table[1:], start=1):\n",
    "                            row_texts = [\n",
    "                                f\"{headers[i]}: {str(cell).strip()}\"\n",
    "                                for i, cell in enumerate(row) if cell and str(cell).strip()\n",
    "                            ]\n",
    "                            if not row_texts: continue\n",
    "                            \n",
    "                            page_content = \" | \".join(row_texts)\n",
    "                            metadata = {\n",
    "                                \"source\": str(file_path), \"file_type\": \"pdf\", \"page_num\": page_num,\n",
    "                                \"content_type\": \"table_row\", \"table_id\": table_idx, \n",
    "                                \"row_index_in_table\": row_idx, \"extraction_method\": \"text\"\n",
    "                            }\n",
    "                            documents.append(Document(page_content=page_content, metadata=metadata))\n",
    "\n",
    "                    # 2. Extract text and chunk it\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text and page_text.strip():\n",
    "                        text_chunks = text_splitter.split_text(page_text)\n",
    "                        for chunk in text_chunks:\n",
    "                            metadata = {\n",
    "                                \"source\": str(file_path), \"file_type\": \"pdf\", \"page_num\": page_num,\n",
    "                                \"content_type\": \"paragraph\", \"extraction_method\": \"text\"\n",
    "                            }\n",
    "                            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during direct PDF parsing for {file_path}: {e}\")\n",
    "\n",
    "        # --- Method 2: OCR Fallback if direct extraction yields little content ---\n",
    "        if not documents or len(\"\".join(d.page_content for d in documents)) < 100:\n",
    "            print(f\"PDF {file_path} has little text, trying OCR...\")\n",
    "            try:\n",
    "                images = convert_from_path(file_path, dpi=300)\n",
    "                for page_num, image in enumerate(images, start=1):\n",
    "                    ocr_text = pytesseract.image_to_string(image, lang='vie+eng')\n",
    "                    if ocr_text and ocr_text.strip():\n",
    "                        text_chunks = text_splitter.split_text(ocr_text)\n",
    "                        for chunk in text_chunks:\n",
    "                            metadata = {\n",
    "                                \"source\": str(file_path), \"file_type\": \"pdf\", \"page_num\": page_num,\n",
    "                                \"content_type\": \"paragraph\", \"extraction_method\": \"ocr\"\n",
    "                            }\n",
    "                            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "            except Exception as e:\n",
    "                print(f\"Error during OCR PDF parsing for {file_path}: {e}\")\n",
    "\n",
    "        return documents\n",
    "    \n",
    "# Parse all PDF files in the folder\n",
    "if __name__ == \"__main__\":\n",
    "    all_documents = []\n",
    "    for file_path in pdf_files:\n",
    "        file_path = Path(file_path)\n",
    "        print(f\"Parsing file: {file_path}\")\n",
    "        documents = parse_pdf(file_path)\n",
    "        all_documents.extend(documents)\n",
    "        #Save temporary results to avoid losing progress\n",
    "        print(f\"Parsed {len(documents)} documents from {file_path}\")\n",
    "    # Save all documents to JSON file with descriptive name\n",
    "    output_file = Path(\"/mnt/d/Techcombank_/chatbot_document/data/output/pdf_documents_parsed.json\")\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in all_documents], f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Total documents created: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf32fe2",
   "metadata": {},
   "source": [
    "# Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24e979",
   "metadata": {},
   "source": [
    "### List files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "323d3877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Word files found: 1\n",
      "Word files found in /mnt/d/Techcombank_/chatbot_document/data/data_real:\n",
      "SO-SÁNH-DN-CÙNG-NGÀNH.docx\n"
     ]
    }
   ],
   "source": [
    "# List all Word files in the folder\n",
    "word_files = glob.glob(os.path.join(folder_path, \"*.docx\")) + glob.glob(os.path.join(folder_path, \"*.doc\"))\n",
    "print(f\"Total Word files found: {len(word_files)}\")\n",
    "print(f\"Word files found in {folder_path}:\")\n",
    "for file in word_files:\n",
    "    print(os.path.basename(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918dab5",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6240a54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: /mnt/d/Techcombank_/chatbot_document/data/data_real/SO-SÁNH-DN-CÙNG-NGÀNH.docx\n",
      "Parsed 6 documents from /mnt/d/Techcombank_/chatbot_document/data/data_real/SO-SÁNH-DN-CÙNG-NGÀNH.docx\n",
      "Total documents created: 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import docx\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def parse_word(file_path: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Optimized Word parser: extracts tables row-by-row and text paragraph-by-paragraph.\n",
    "    \"\"\"\n",
    "    # Create a text splitter for chunking large texts\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    documents = []\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "\n",
    "        # 1. Extract tables first\n",
    "        for table_idx, table in enumerate(doc.tables):\n",
    "            if not table.rows:\n",
    "                continue\n",
    "            \n",
    "            # Assume the first row is the header\n",
    "            headers = [cell.text.strip() for cell in table.rows[0].cells]\n",
    "            \n",
    "            # Iterate over data rows\n",
    "            for row_idx, row in enumerate(table.rows[1:], start=1):\n",
    "                row_texts = []\n",
    "                for i, cell in enumerate(row.cells):\n",
    "                    cell_text = cell.text.strip()\n",
    "                    if cell_text:\n",
    "                        # Use header if available, otherwise use column index\n",
    "                        header = headers[i] if i < len(headers) else f\"col_{i}\"\n",
    "                        row_texts.append(f\"{header}: {cell_text}\")\n",
    "                \n",
    "                if not row_texts: continue\n",
    "\n",
    "                page_content = \" | \".join(row_texts)\n",
    "                metadata = {\n",
    "                    \"source\": str(file_path),\n",
    "                    \"file_type\": \"word\",\n",
    "                    \"content_type\": \"table_row\",\n",
    "                    \"table_id\": table_idx,\n",
    "                    \"row_index_in_table\": row_idx\n",
    "                }\n",
    "                documents.append(Document(page_content=page_content, metadata=metadata))\n",
    "        \n",
    "        # 2. Extract and chunk paragraph text\n",
    "        # The doc.paragraphs object intelligently excludes text within tables.\n",
    "        full_text = \"\\n\\n\".join(\n",
    "            para.text.strip() for para in doc.paragraphs if para.text.strip()\n",
    "        )\n",
    "        \n",
    "        if full_text:\n",
    "            text_chunks = text_splitter.split_text(full_text)\n",
    "            for chunk in text_chunks:\n",
    "                metadata = {\n",
    "                    \"source\": str(file_path),\n",
    "                    \"file_type\": \"word\",\n",
    "                    \"content_type\": \"paragraph\"\n",
    "                }\n",
    "                documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing Word file (optimized) {file_path}: {e}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Parse all Word files in the folder\n",
    "if __name__ == \"__main__\":\n",
    "    all_documents = []\n",
    "    for file_path in word_files:\n",
    "        file_path = Path(file_path)\n",
    "        print(f\"Parsing file: {file_path}\")\n",
    "        documents = parse_word(file_path)\n",
    "        all_documents.extend(documents)\n",
    "        #Save temporary results to avoid losing progress\n",
    "        print(f\"Parsed {len(documents)} documents from {file_path}\")\n",
    "    # Save all documents to JSON file with descriptive name\n",
    "    output_file = Path(\"/mnt/d/Techcombank_/chatbot_document/data/output/word_documents_parsed.json\")\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in all_documents], f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Total documents created: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc8dc2",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14917e",
   "metadata": {},
   "source": [
    "#### List files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e99e667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text files found: 0\n",
      "Text files found in /mnt/d/Techcombank_/chatbot_document/data/data_real:\n"
     ]
    }
   ],
   "source": [
    "# List all text files in the folder\n",
    "text_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "print(f\"Total text files found: {len(text_files)}\")\n",
    "print(f\"Text files found in {folder_path}:\")\n",
    "for file in text_files:\n",
    "    print(os.path.basename(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6bb42",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b91ce4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents created: 0\n"
     ]
    }
   ],
   "source": [
    "def parse_text(file_path: Path) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Optimized Text parser: reads the entire file and splits it into manageable\n",
    "        text chunks using the class's text_splitter.\n",
    "        \"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        )\n",
    "        documents = []\n",
    "        content = \"\"\n",
    "        encoding_used = \"utf-8\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"UTF-8 decoding failed for {file_path}. Trying latin-1.\")\n",
    "            encoding_used = \"latin-1\"\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                    content = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading text file {file_path} with latin-1: {e}\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading text file {file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "        if content.strip():\n",
    "            text_chunks = text_splitter.split_text(content)\n",
    "            for i, chunk in enumerate(text_chunks):\n",
    "                metadata = {\n",
    "                    \"source\": str(file_path),\n",
    "                    \"file_type\": \"text\",\n",
    "                    \"content_type\": \"text_chunk\",\n",
    "                    \"chunk_id\": i,\n",
    "                    \"encoding\": encoding_used\n",
    "                }\n",
    "                documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "# Parse all text files in the folder\n",
    "if __name__ == \"__main__\":\n",
    "    all_documents = []\n",
    "    for file_path in text_files:\n",
    "        file_path = Path(file_path)\n",
    "        print(f\"Parsing file: {file_path}\")\n",
    "        documents = parse_text(file_path)\n",
    "        all_documents.extend(documents)\n",
    "        #Save temporary results to avoid losing progress\n",
    "        print(f\"Parsed {len(documents)} documents from {file_path}\")\n",
    "    # Save all documents to JSON file with descriptive name\n",
    "    output_file = Path(\"/mnt/d/Techcombank_/chatbot_document/data/output/text_documents_parsed.json\")\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in all_documents], f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Total documents created: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ecece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
